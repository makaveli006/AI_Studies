{"cells":[{"cell_type":"markdown","id":"bdc87fa7","metadata":{"id":"bdc87fa7"},"source":["# Computer Vision Basics"]},{"cell_type":"markdown","id":"db477d98-a174-4334-96ca-03d508dfbe57","metadata":{"id":"db477d98-a174-4334-96ca-03d508dfbe57"},"source":["### Haar Cascade Classifier\n","It is an Object Detection Algorithm used to identify faces in an image or a real time video. The algorithm uses edge or line detection features proposed by Viola and Jones in their research paper “Rapid Object Detection using a Boosted Cascade of Simple Features” published in 2001. The algorithm is given a lot of positive images consisting of faces, and a lot of negative images not consisting of any face to train on them.\n","\n","### Features\n","![ScreenShot_02-08-2023_09_58_09.png](attachment:8bba50c7-3e39-4889-b95a-bf7da63831cb.png)\n","\n","The first contribution to the research was the introduction of the haar features shown above. These features on the image makes it easy to find out the edges or the lines in the image, or to pick areas where there is a sudden change in the intensities of the pixels.\n","\n","![ScreenShot_02-08-2023_09_59_45.png](attachment:5a3dd16b-b4ce-4273-baa9-1b3c1c9401ac.png)\n","\n","A sample calculation of Haar value from a rectangular image section has been shown here. The darker areas in the haar feature are pixels with values 1, and the lighter areas are pixels with values 0. Each of these is responsible for finding out one particular feature in the image. Such as an edge, a line or any structure in the image where there is a sudden change of intensities. For ex. in the image above, the haar feature can detect a vertical edge with darker pixels at its right and lighter pixels at its left.\n","\n","The objective here is to find out the sum of all the image pixels lying in the darker area of the haar feature and the sum of all the image pixels lying in the lighter area of the haar feature. And then find out their difference. Now if the image has an edge separating dark pixels on the right and light pixels on the left, then the haar value will be closer to 1. That means, we say that there is an edge detected if the haar value is closer to 1. In the example above, there is no edge as the haar value is far from\n","\n","This is just one representation of a particular haar feature separating a vertical edge. Now there are other haar features as well, which will detect edges in other directions and any other image structures. To detect an edge anywhere in the image, the haar feature needs to traverse the whole image.1\n","![1_BpHwuCr9q9eldVKzUcFkwA.gif](attachment:bf267c47-3488-4aae-ac3b-6106057714b7.gif)\n","\n","The haar feature continuously traverses from the top left of the image to the bottom right to search for the particular feature. This is just a representation of the whole concept of the haar feature traversal. In its actual work, the haar feature would traverse pixel by pixel in the image. Also all possible sizes of the haar features will be applied.\n","\n","Depending on the feature each one is looking for, these are broadly classified into three categories. The first set of two rectangle features are responsible for finding out the edges in a horizontal or in a vertical direction (as shown above). The second set of three rectangle features are responsible for finding out if there is a lighter region surrounded by darker regions on either side or vice-versa. The third set of four rectangle features are responsible for finding out change of pixel intensities across diagonals.\n","\n","Now, the haar features traversal on an image would involve a lot of mathematical calculations. As we can see for a single rectangle on either side, it involves 18 pixel value additions (for a rectangle enclosing 18 pixels). Imagine doing this for the whole image with all sizes of the haar features. This would be a hectic operation even for a high performance machine\n","![1_K2r9aTsaU-spymgjcMCWAA.gif](attachment:7d2b2677-1d6e-43a0-8487-1c6da8f40476.gif)\n","\n","To tackle this, they introduced another concept known as The Integral Image to perform the same operation. An Integral Image is calculated from the Original Image in such a way that each pixel in this is the sum of all the pixels lying in its left and above in the Original Image. The calculation of a pixel in the Integral Image can be seen in the above GIF. The last pixel at the bottom right corner of the Integral Image will be the sum of all the pixels in the Original Image.\n","\n","![ScreenShot_02-08-2023_10_04_31.png](attachment:9b609619-c6e8-4491-9f1b-03a4d8790f31.png)\n","\n","With the Integral Image, only 4 constant value additions are needed each time for any feature size (with respect to the 18 additions earlier). This reduces the time complexity of each addition gradually, as the number of additions does not depend on the number of pixels enclosed anymore.\n","\n","In the above image, there is no edge in the vertical direction as the haar value is -0.02, which is very far from 1. Let’s see one more example, where there might be an edge present in the image.![ScreenShot_02-08-2023_10_06_23.png](attachment:f3a5fb2b-a03e-4af4-933d-cabc21b10a98.png)\n","\n","Again repeating the same calculation done above, but this time just to see what haar value is calculated when there is a sudden change of intensities moving from left to right in a vertical direction. The haar value here is 0.54, which is closer to 1 in comparison to the case earlier.\n","![1_gQdlL1v88PVsIaSyfV_Gkw.gif](attachment:ecb5daae-eaaa-4402-bdef-af7bc09644aa.gif)\n","\n","![1_CMDXEnq9_XkBaT_3CZEh1Q.gif](attachment:eaaa92ae-b7a0-44cf-abd2-aa2e3f7cdd1f.gif)\n","\n","\n","..\n","\n"]},{"cell_type":"markdown","id":"96f266dc-a617-4a9d-9fb3-58ce673cff23","metadata":{"id":"96f266dc-a617-4a9d-9fb3-58ce673cff23"},"source":["# BASICS"]},{"cell_type":"code","execution_count":1,"id":"b2b69a88","metadata":{"id":"b2b69a88","executionInfo":{"status":"ok","timestamp":1691388984281,"user_tz":-330,"elapsed":6808,"user":{"displayName":"faris k","userId":"05801493688378815047"}}},"outputs":[],"source":["import cv2"]},{"cell_type":"code","execution_count":3,"id":"ed6c5a57","metadata":{"id":"ed6c5a57","executionInfo":{"status":"ok","timestamp":1691389016136,"user_tz":-330,"elapsed":1639,"user":{"displayName":"faris k","userId":"05801493688378815047"}}},"outputs":[],"source":["image = cv2.imread('img.jpg')\n"]},{"cell_type":"code","execution_count":4,"id":"6c14e740","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"6c14e740","executionInfo":{"status":"error","timestamp":1691389031236,"user_tz":-330,"elapsed":392,"user":{"displayName":"faris k","userId":"05801493688378815047"}},"outputId":"773e41a9-4d98-45f5-983a-5b0a9a0294ec"},"outputs":[{"output_type":"error","ename":"DisabledFunctionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-f8c98717f98e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_flower'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"],"errorDetails":{"actions":[{"action":"open_snippet","actionText":"Search Snippets for cv2.imshow","snippetFilter":"cv2.imshow"}]}}],"source":["cv2.imshow('image_flower',image)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":5,"id":"09803702","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09803702","executionInfo":{"status":"ok","timestamp":1691389050757,"user_tz":-330,"elapsed":417,"user":{"displayName":"faris k","userId":"05801493688378815047"}},"outputId":"656d4864-8a68-40d1-f6e7-e600b5fd415b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}],"source":["cv2.imwrite('2.jpg',image)\n"]},{"cell_type":"code","execution_count":6,"id":"0b7092c2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"0b7092c2","executionInfo":{"status":"error","timestamp":1691389058549,"user_tz":-330,"elapsed":422,"user":{"displayName":"faris k","userId":"05801493688378815047"}},"outputId":"c34dfb3a-59cd-4054-be2f-ea667f9ffd57"},"outputs":[{"output_type":"error","ename":"DisabledFunctionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-339b3994b31b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grey'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"],"errorDetails":{"actions":[{"action":"open_snippet","actionText":"Search Snippets for cv2.imshow","snippetFilter":"cv2.imshow"}]}}],"source":["image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n","cv2.imshow('grey',image)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":7,"id":"4ce6a20b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ce6a20b","executionInfo":{"status":"ok","timestamp":1691389060592,"user_tz":-330,"elapsed":10,"user":{"displayName":"faris k","userId":"05801493688378815047"}},"outputId":"f17d2f84-c9d6-431e-d592-b576803a2c43"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["cv2.imwrite('Edge.jpg',cv2.Canny(image,200,300))"]},{"cell_type":"code","execution_count":8,"id":"cfda9907","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"cfda9907","executionInfo":{"status":"error","timestamp":1691389063500,"user_tz":-330,"elapsed":634,"user":{"displayName":"faris k","userId":"05801493688378815047"}},"outputId":"b0575500-4bef-4b1c-ccb7-6a49bf4fb731"},"outputs":[{"output_type":"error","ename":"DisabledFunctionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-b8974bb3a034>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Edges'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Edge.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"],"errorDetails":{"actions":[{"action":"open_snippet","actionText":"Search Snippets for cv2.imshow","snippetFilter":"cv2.imshow"}]}}],"source":["cv2.imshow('Edges', cv2.imread('Edge.jpg'))\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","id":"73b68796","metadata":{"id":"73b68796"},"source":["### Face Detection"]},{"cell_type":"code","execution_count":null,"id":"9cca3a9a","metadata":{"id":"9cca3a9a"},"outputs":[],"source":["import cv2\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"a6925749","metadata":{"id":"a6925749"},"outputs":[],"source":["face_detection =cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"]},{"cell_type":"code","execution_count":null,"id":"be48170d","metadata":{"id":"be48170d"},"outputs":[],"source":["img = cv2.imread('face.jpg')\n"]},{"cell_type":"code","execution_count":null,"id":"a8ee2db3","metadata":{"id":"a8ee2db3"},"outputs":[],"source":["grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"]},{"cell_type":"markdown","id":"1a0ee63b","metadata":{"id":"1a0ee63b"},"source":["![ScreenShot_26-07-2023_13_10_34.png](attachment:ScreenShot_26-07-2023_13_10_34.png)"]},{"cell_type":"markdown","id":"8f49ca37","metadata":{"id":"8f49ca37"},"source":["**MultiScale** detects objects of different sizes in the input image and returns rectangles positioned on the faces. The first argument is the image, the second is the scalefactor (how much the image size will be reduced at each image scale), and the third is the minNeighbors (how many neighbors each rectangle should have). The values of 1.3 and 5 are based on experimenting and choosing those that worked best."]},{"cell_type":"code","execution_count":null,"id":"92d8e287","metadata":{"id":"92d8e287"},"outputs":[],"source":["faces = face_detection.detectMultiScale(grey, 1.3, 5)"]},{"cell_type":"markdown","id":"05305bc2","metadata":{"id":"05305bc2"},"source":["**for (x, y, w, h) in faces**: We will loop through each rectangle (each face detected) using its coordinates generated by the function we discussed above.\n","\n","**cv2.rectangle(frame, (x,y), (x+w, y+h), (255, 0, 0), 2)** We are drawing the rectangle in the original image that we are capturing in the camera (last frame). the (255,0,0) is the color of the frame in RGB. The last parameter (2) is the thickness of the rectangle. x is the horizontal initial position, w is the width, y is the vertical initial position, h is the height.\n","\n","**roi_gray = gray[y:y+h, x:x+w]** Here we are setting roi_gray to be our region of interest. That’s where we will look for the eyes.\n","\n","**roi_color = frame[y:y+h, x:x+w]** We are getting the region of interest in the original frame (colored, not black & white)."]},{"cell_type":"code","execution_count":null,"id":"6e89b301","metadata":{"id":"6e89b301","outputId":"a389bb5a-8a16-4f7c-c579-28f721a8ef70"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["\n","for (x,y,w,h) in faces:\n","    img = cv2.rectangle(img,(x,y),(x+w, y+h),(255,0,0),3)\n","cv2.imwrite('Face_AB.jpg',img)"]},{"cell_type":"code","execution_count":null,"id":"9fefdc73","metadata":{"id":"9fefdc73"},"outputs":[],"source":["cv2.imshow('image_flower',img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","id":"e73e1788-ea72-4f6b-859c-04838d7cdfb6","metadata":{"id":"e73e1788-ea72-4f6b-859c-04838d7cdfb6"},"source":["# DETECT FACE FROM VIDEO"]},{"cell_type":"code","execution_count":null,"id":"88546173","metadata":{"id":"88546173"},"outputs":[],"source":["# Necessary imports\n","import cv2\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"b173d31e","metadata":{"id":"b173d31e"},"outputs":[],"source":["# Loading the image\n","img = cv2.imread(\"girl.jpg\")\n","cv2.imshow('Image', img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"id":"9a30c7d1","metadata":{"id":"9a30c7d1"},"outputs":[],"source":["face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n","eye_cascade = cv2.CascadeClassifier(\"haarcascade_eye.xml\")\n","smile_cascade = cv2.CascadeClassifier('haarcascade_smile.xml')"]},{"cell_type":"code","execution_count":null,"id":"a8f5792d","metadata":{"id":"a8f5792d"},"outputs":[],"source":["gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","# Creating an object faces\n","faces= face_cascade.detectMultiScale (gray, 1.3, 5)\n","# Drawing rectangle around the face\n","for(x , y,  w,  h) in faces:\n","    cv2.rectangle(img, (x,y) ,(x+w, y+h), (0,255,0), 3)\n",""]},{"cell_type":"code","execution_count":null,"id":"37452da0","metadata":{"id":"37452da0"},"outputs":[],"source":["cv2.imshow('Image', img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"id":"b92131d4","metadata":{"id":"b92131d4"},"outputs":[],"source":["# Creating two objects of interest\n","roi_gray=gray[y:(y+h), x:(x+w)]\n","roi_color=img[y:(y+h), x:(x+w)]"]},{"cell_type":"code","execution_count":null,"id":"eeaa299f","metadata":{"id":"eeaa299f"},"outputs":[],"source":["eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 10)\n","for (x_eye, y_eye, w_eye, h_eye) in eyes:\n","  cv2.rectangle(roi_color,(x_eye, y_eye),(x_eye+w_eye, y_eye+h_eye), (0, 0, 255), 3)\n"]},{"cell_type":"code","execution_count":null,"id":"ac451f6e","metadata":{"id":"ac451f6e"},"outputs":[],"source":["cv2.imshow('Image', img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"id":"d7c66261","metadata":{"id":"d7c66261","outputId":"ea65984d-372e-4e09-b77c-c502ff2fc1f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["smile detected\n"]}],"source":["   # detecting smile within the face roi\n","smiles = smile_cascade.detectMultiScale(roi_gray, 1.8, 20)\n","if len(smiles) > 0:\n","    print(\"smile detected\")\n","    for (sx, sy, sw, sh) in smiles:\n","        cv2.rectangle(roi_color, (sx, sy), ((sx + sw), (sy + sh)), (255, 0, 130), 2)\n","        cv2.putText(roi_color, \"smile\", (sx, sy),\n","        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","else:\n","    print(\"smile not detected\")"]},{"cell_type":"code","execution_count":null,"id":"06dcebf1","metadata":{"id":"06dcebf1"},"outputs":[],"source":["cv2.imshow('Image', img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"id":"10add0b0","metadata":{"id":"10add0b0","outputId":"a0eca079-ef5d-4e68-f687-ef3148c46cc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n","smile not detected\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[56], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmile not detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m,img)\n\u001b[1;32m---> 39\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xff\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","import cv2\n","\n","# multiple cascades: https://github.com/Itseez/opencv/tree/master/data/haarcascades\n","\n","#https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n","face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","#https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_eye.xml\n","eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n","smile_cascade = cv2.CascadeClassifier('haarcascade_smile.xml')\n","\n","cap = cv2.VideoCapture(0)\n","\n","while 1:\n","    ret, img = cap.read()\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","\n","    for (x,y,w,h) in faces:\n","        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n","        roi_gray = gray[y:y+h, x:x+w]\n","        roi_color = img[y:y+h, x:x+w]\n","\n","        eyes = eye_cascade.detectMultiScale(roi_gray)\n","        for (ex,ey,ew,eh) in eyes:\n","            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n","\n","        smiles = smile_cascade.detectMultiScale(roi_gray, 1.8, 20)\n","        if len(smiles) > 0:\n","            print(\"smile detected\")\n","            for (sx, sy, sw, sh) in smiles:\n","                cv2.rectangle(roi_color, (sx, sy), ((sx + sw), (sy + sh)), (255, 0, 130), 2)\n","                cv2.putText(roi_color, \"smile\", (sx, sy),\n","                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","        else:\n","            print(\"smile not detected\")\n","\n","    cv2.imshow('img',img)\n","    k = cv2.waitKey(30) & 0xff\n","    if k == 27:\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","id":"05267098","metadata":{"id":"05267098"},"source":["**cap = cv2.VideoCapture(0)** Here we are accessing your camera. When the parameter is 0, we are accessing an internal camera from your computer. When the parameter is 1, we are accessing an external camera that is plugged on your computer.\n","\n","**while True**: We will keep running the detect function with the code that follows while the camera is opened (until we close it using a key that we will define).\n","\n","**_, frame = video_capture.read()** the read() function will return 2 objects, but we are interested only in the latest one, that is the last frame from the camera. So we are ignoring the first object _, and naming the second as frame, that we will use to feed our detect() function in a second.\n","\n","**gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)** We are getting the frame we just read from the function above and converting it to grayscale. We are naming it gray and we will use to feed our detect function as well.\n","\n","**k= cv2.waiKey(1) & 0xFF == ord('q')** : break This piece of code will stop the program when you press q on the keyboard."]},{"cell_type":"code","execution_count":null,"id":"b0396dfa","metadata":{"id":"b0396dfa"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}