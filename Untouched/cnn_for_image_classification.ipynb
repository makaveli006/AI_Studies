{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mesushan/CNN-for-image-Classification/blob/master/cnn_for_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_NcIIc_P1xro"
   },
   "source": [
    "## STEP 1: SELECT RIGHT VERSION OF TENSORFLOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "EE2zTcHEJUGW",
    "outputId": "131f7e73-749e-4ef2-a008-6ace19c2e821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78RhkOKM3Ld8"
   },
   "source": [
    "## STEP 2: CLONE GITHUB PROJECT (FOR EASY ACCESS TO DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9QCWniNOVt-5",
    "outputId": "4262e2f6-6f4b-448b-d541-c5199b7c3c48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'CNN-for-image-Classification' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mesushan/CNN-for-image-Classification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Un6b6Tdxeccs",
    "outputId": "7dbcb1f9-ce3f-4774-89a7-9e1618eb748a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-for-image-Classification  drive  sample_data\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9S_E5AgNJkNA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bS6MNMEzKBxs"
   },
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RWO-fud3jLD"
   },
   "source": [
    "## Step 3: ADDING CONVOLUTION LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1p3ZYBEKnQD"
   },
   "outputs": [],
   "source": [
    "# 32 feature detectors with 3*3 dimensions so the convolution layer compose of 32 feature maps\n",
    "# 128 by 128 dimensions with colored image(3 channels)  (tensorflow backend)\n",
    "input_size = (128, 128)\n",
    "model.add(tf.keras.layers.Convolution2D(32, 3, 3, input_shape = (*input_size, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8GGnBAso3zyT"
   },
   "source": [
    "## STEP 4: ADDING POOLING LAYER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEiPkwwULNeC"
   },
   "outputs": [],
   "source": [
    "# reduce the size of feature maps and therefore reduce the number of nodes in the future fully connected layer (reduce time complexity, less compute intense without losing the performace). 2 by 2 deminsion is the recommended option\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X9HXpcCV4xFU"
   },
   "source": [
    "## STEP 5: ADDING SECOND CONVOLUTION LAYER WITH POOLIING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yoh6opl-MPpA"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Convolution2D(32, 3, 3, activation = 'relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lvp8NS-85iWn"
   },
   "source": [
    "## STEP 6: ADDING FLATTENING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryYAi7KIMd6n"
   },
   "outputs": [],
   "source": [
    "# flatten all the feature maps in the pooling layer into single vector\n",
    "model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XE8mbDLq54_p"
   },
   "source": [
    "## STEP 7: ADDING A FULLY CONNECTED LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIo_G3EeMlAo"
   },
   "outputs": [],
   "source": [
    "# making classic ann which compose of fully connected layers\n",
    "# number of nodes in hidden layer (output_dim) (common practice is to take the power of 2)\n",
    "model.add(tf.keras.layers.Dense(units = 64, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sbrjo20M6CXH"
   },
   "source": [
    "## STEP 8: COMPILING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jwlI36vMvJW"
   },
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CWYiHqn6SiJ"
   },
   "source": [
    "## STEP 9: FITTING THE CNN TO THE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ggxWEo0-M1h3",
    "outputId": "ed4717df-e4b5-426f-a81d-d575554d0842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/35\n",
      "250/250 [==============================] - 324s 1s/step - loss: 0.6890 - accuracy: 0.5357 - val_loss: 0.6703 - val_accuracy: 0.6025\n",
      "Epoch 2/35\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.6662 - accuracy: 0.5870 - val_loss: 0.6357 - val_accuracy: 0.6420\n",
      "Epoch 3/35\n",
      "250/250 [==============================] - 25s 102ms/step - loss: 0.6360 - accuracy: 0.6366 - val_loss: 0.6005 - val_accuracy: 0.6950\n",
      "Epoch 4/35\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.6070 - accuracy: 0.6620 - val_loss: 0.5836 - val_accuracy: 0.6985\n",
      "Epoch 5/35\n",
      "250/250 [==============================] - 27s 108ms/step - loss: 0.5778 - accuracy: 0.6965 - val_loss: 0.5644 - val_accuracy: 0.7080\n",
      "Epoch 6/35\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.5664 - accuracy: 0.7046 - val_loss: 0.5532 - val_accuracy: 0.7200\n",
      "Epoch 7/35\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.5473 - accuracy: 0.7281 - val_loss: 0.5345 - val_accuracy: 0.7315\n",
      "Epoch 8/35\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.5520 - accuracy: 0.7155 - val_loss: 0.5327 - val_accuracy: 0.7490\n",
      "Epoch 9/35\n",
      "250/250 [==============================] - 25s 101ms/step - loss: 0.5314 - accuracy: 0.7297 - val_loss: 0.5288 - val_accuracy: 0.7400\n",
      "Epoch 10/35\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.5223 - accuracy: 0.7408 - val_loss: 0.5092 - val_accuracy: 0.7620\n",
      "Epoch 11/35\n",
      "250/250 [==============================] - 28s 111ms/step - loss: 0.5181 - accuracy: 0.7398 - val_loss: 0.5023 - val_accuracy: 0.7650\n",
      "Epoch 12/35\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.5115 - accuracy: 0.7510 - val_loss: 0.5082 - val_accuracy: 0.7605\n",
      "Epoch 13/35\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.5133 - accuracy: 0.7516 - val_loss: 0.5013 - val_accuracy: 0.7670\n",
      "Epoch 14/35\n",
      "250/250 [==============================] - 28s 113ms/step - loss: 0.5008 - accuracy: 0.7560 - val_loss: 0.5147 - val_accuracy: 0.7520\n",
      "Epoch 15/35\n",
      "250/250 [==============================] - 27s 106ms/step - loss: 0.4966 - accuracy: 0.7585 - val_loss: 0.5097 - val_accuracy: 0.7540\n",
      "Epoch 16/35\n",
      "250/250 [==============================] - 26s 104ms/step - loss: 0.4955 - accuracy: 0.7616 - val_loss: 0.4919 - val_accuracy: 0.7670\n",
      "Epoch 17/35\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.4860 - accuracy: 0.7696 - val_loss: 0.4804 - val_accuracy: 0.7855\n",
      "Epoch 18/35\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.4916 - accuracy: 0.7676 - val_loss: 0.5078 - val_accuracy: 0.7555\n",
      "Epoch 19/35\n",
      "250/250 [==============================] - 27s 106ms/step - loss: 0.4763 - accuracy: 0.7715 - val_loss: 0.4835 - val_accuracy: 0.7810\n",
      "Epoch 20/35\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.4765 - accuracy: 0.7688 - val_loss: 0.5697 - val_accuracy: 0.7090\n",
      "Epoch 21/35\n",
      "250/250 [==============================] - 26s 105ms/step - loss: 0.4802 - accuracy: 0.7720 - val_loss: 0.4705 - val_accuracy: 0.7865\n",
      "Epoch 22/35\n",
      "250/250 [==============================] - 27s 109ms/step - loss: 0.4765 - accuracy: 0.7776 - val_loss: 0.4806 - val_accuracy: 0.7805\n",
      "Epoch 23/35\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.4668 - accuracy: 0.7747 - val_loss: 0.5068 - val_accuracy: 0.7670\n",
      "Epoch 24/35\n",
      "250/250 [==============================] - 27s 107ms/step - loss: 0.4637 - accuracy: 0.7786 - val_loss: 0.4972 - val_accuracy: 0.7720\n",
      "Epoch 25/35\n",
      "250/250 [==============================] - 26s 103ms/step - loss: 0.4656 - accuracy: 0.7796 - val_loss: 0.5167 - val_accuracy: 0.7420\n",
      "Epoch 26/35\n",
      "250/250 [==============================] - 28s 112ms/step - loss: 0.4645 - accuracy: 0.7790 - val_loss: 0.4659 - val_accuracy: 0.7795\n",
      "Epoch 27/35\n",
      "250/250 [==============================] - 28s 114ms/step - loss: 0.4541 - accuracy: 0.7865 - val_loss: 0.4942 - val_accuracy: 0.7710\n",
      "Epoch 28/35\n",
      "130/250 [==============>...............] - ETA: 11s - loss: 0.4566 - accuracy: 0.7894"
     ]
    }
   ],
   "source": [
    "# image augmentation technique to enrich our dataset(training set) without adding more images so get good performance  results with little or no overfitting even with the small amount of images\n",
    "# used from keras documentation (flow_from_directory method)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size = 32\n",
    "# image augmentation part\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# create training set\n",
    "# wanna get higher accuracy -> inccrease target_size\n",
    "training_set = train_datagen.flow_from_directory('C:/Users/Leena Ali/Documents/iDatalytics/Artificial Intelligence _ Data Science/Programs/Artificial Neural Networks/CNN/CNN-for-image-Classification-master/dataset/training_set',\n",
    "                                                 target_size = input_size,\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "# create test set\n",
    "# wanna get higher accuracy -> inccrease target_size\n",
    "test_set = test_datagen.flow_from_directory('C:/Users/Leena Ali/Documents/iDatalytics/Artificial Intelligence _ Data Science/Programs/Artificial Neural Networks/CNN/CNN-for-image-Classification-master/dataset/test_set',\n",
    "                                            target_size = input_size,\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "# fit the cnn model to the trainig set and testing it on the test set\n",
    "model.fit(training_set,\n",
    "          steps_per_epoch = 8000/batch_size,\n",
    "          epochs = 35,\n",
    "          validation_data = test_set,\n",
    "          validation_steps = 2000/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FablVLKI1ujz"
   },
   "source": [
    "## STEP 10: MAKING NEW *PREDICTIONS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkpQ9_Bbdgug"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.utils as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OARxcBiKqhaP"
   },
   "outputs": [],
   "source": [
    "test_image = image.load_img('C:/Users/Leena Ali/Documents/iDatalytics/Artificial Intelligence _ Data Science/Programs/Artificial Neural Networks/CNN/CNN-for-image-Classification-master/dataset/single_prediction/2.jpg', target_size= input_size)\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yephYECUq3BH",
    "outputId": "aa6219cf-0719-4857-c100-becf44791bec"
   },
   "outputs": [],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KwfR3qnDr2D2"
   },
   "outputs": [],
   "source": [
    "if result [0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "T-LuaKWdsKIn",
    "outputId": "a91c3848-85c5-4f31-b968-5ec070d6d1fa"
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNTVV8xoVuMN"
   },
   "source": [
    "### correct prediction made :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpVRkKU1Mo1d"
   },
   "source": [
    "#### The model shows the accuracy of 79.34 percent for the training set and 79.55 for the validation set. Can be improved with further parameter tuning if needed. ex. increasing the epochs, adding more convolution layer, changing input size of image, etc."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMH4CbnVnrPqPPnZWyDXl/2",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "cnn_for_image_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
